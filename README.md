```markdown
# ğŸš€ Local ChatGPT â€” Powered by Ollama + Python (Streaming + PDF RAG + Embeddings + FAISS)

![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Flask](https://img.shields.io/badge/Flask-Web%20Framework-green)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-orange)
![FAISS](https://img.shields.io/badge/FAISS-Vector%20Search-purple)
![License](https://img.shields.io/badge/License-MIT-yellow)
![Stars](https://img.shields.io/github/stars/manojrammurthy/ollama-local-chatgpt?style=social)

A **fully local ChatGPT alternative** with:

âœ” ChatGPT-style UI  
âœ” Local LLMs via Ollama (Phi-3, LLaMA-3, Mistral, etc.)  
âœ” PDF Upload + RAG  
âœ” Real-time streaming responses  
âœ” Embeddings using `nomic-embed-text`  
âœ” FAISS vector search  
âœ” No cloud, no API keys â€” **100% offline**

Built using **Flask + TailwindCSS + Ollama + FAISS + PyMuPDF**.

---

---

# âœ¨ Features

| Feature           | Description                                              |
| ----------------- | -------------------------------------------------------- |
| ğŸ§  Local LLM Chat | Uses any Ollama model (Phi-3, LLaMA-3, Mistral, Gemmaâ€¦ ) |
| ğŸ“„ PDF Upload     | Load documents and ask questions from them               |
| ğŸ” RAG            | FAISS-powered retrieval from PDF chunks                  |
| ğŸ§¬ Embeddings     | Uses `nomic-embed-text` for dense vector embeddings      |
| ğŸš€ Live Streaming | Real-time token-by-token streaming like ChatGPT          |
| ğŸ¨ Modern UI      | TailwindCSS dark-mode chat interface                     |
| ğŸ” Model Selector | Switch Ollama models dynamically                         |
| ğŸ’¾ Chat Memory    | Auto-saved in browser localStorage                       |
| ğŸ” 100% Offline   | No external API calls â€” everything runs on your laptop   |

---

# ğŸ§  Architecture Overview

```
Frontend (HTML + TailwindCSS)
        â†“
Flask API (Python)
        â†“
Ollama Chat Models (phi3, llama3, mistralâ€¦)
        â†“
Ollama Embedding Model (nomic-embed-text)
        â†“
FAISS Vector Search
        â†“
PDF Question Answering (RAG)
```

---

# ğŸ“ Project Structure

```
ollama_web_chat/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”‚
â””â”€â”€ static/   (optional for CSS/JS assets)
```

---

# âš™ï¸ Installation

## 1ï¸âƒ£ Install Ollama

Linux/macOS:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Windows:
Download from [https://ollama.com/download](https://ollama.com/download)

---

## 2ï¸âƒ£ Pull required Ollama models

```bash
ollama pull phi3
ollama pull llama3
ollama pull mistral
ollama pull nomic-embed-text
```

---

## 3ï¸âƒ£ Install Python dependencies

```bash
pip install -r requirements.txt
```

---

## 4ï¸âƒ£ Run the Flask app

```bash
python app.py
```

Then open:

```
http://localhost:5000
```

---

# ğŸ§¬ Embeddings Explained

This project uses:

```
nomic-embed-text
```

Each chunk of your PDF is converted into a vector (embedding).
These vectors are stored in a **FAISS index**.

During a question:

1. User question â†’ embedded
2. Vector compared against FAISS index
3. Top-k relevant chunks returned
4. Passed to LLM as RAG prompt
5. LLM answers using context

This gives extremely accurate answers for:

* Annual reports
* Research papers
* Legal documents
* Technical PDFs
* Policies

---

# ğŸ“„ PDF RAG Flow

```
PDF â†’ Text Extraction â†’ Chunking â†’ Embedding â†’ FAISS Index
                                 â†‘
                           User Question
                                 â†“
                            Vector Search
                                 â†“
                          Relevant Chunks
                                 â†“
                         LLM generates answer
```

---

# ğŸ” Switching Models

The UI pulls all locally installed models:

```
/models â†’ from Ollama tags API
```

You can switch between:

* phi3
* mistral
* llama3
* codellama
* gemma
* your custom models

Every change clears chat history automatically.

---

# ğŸš€ Roadmap

### âœ” v1.0 (Current)

* Local chat
* PDF embedding + FAISS
* Model selection
* Streaming
* Modern UI

### ğŸ”œ v1.1

* Sidebar multi-threaded chat
* Voice input (Whisper)
* Export conversation
* Multi-PDF knowledgebase
* Dark/light toggle

### ğŸ”® v2.0

* Custom embedding model fine-tuning
* Workspace mode (knowledge graphs)
* Browser extension version
* Desktop app (Electron or PyInstaller)

---

# ğŸ¤ Contributing

Pull requests welcome!
If you want to add a feature, open an issue first.

---

# â­ Support the Project

If you find this useful, please give this repo a â­ on GitHub.
It helps more people discover offline LLM tools.

---

# ğŸ“„ License

This project is licensed under the **MIT License**.

````

